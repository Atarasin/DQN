{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "uuid": "a30d5dc7-a3ec-41a2-bf50-fcab734be0ed"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import gym\n",
    "import numpy as np\n",
    "from random import random\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "# import visdom\n",
    "import time\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "e2f741ef-dfc9-4a7d-8602-0cb9d61486d7"
   },
   "source": [
    "## 经验回放类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "uuid": "3b10a8c6-8939-4c90-843e-fb36d3c96430"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, pool_size=1000000, frame_history_len=4):\n",
    "        # the total size of the Buffer\n",
    "        self.pool_size = pool_size\n",
    "        # the number of memories of each observation\n",
    "        self.frame_history_len = frame_history_len\n",
    "\n",
    "        # stored memories (the list of dict)\n",
    "        self.memories = None\n",
    "        self.obs_shape = None\n",
    "\n",
    "        self.number_of_memories = 0\n",
    "        self.next_idx = 0\n",
    "\n",
    "    def _check_idx(self, cur_idx):\n",
    "        \"\"\"\n",
    "        if memory pool cannot meet \"frame_history_len\" frames, then padding 0.\n",
    "\n",
    "        situation 1: cur_idx < frame_history_len and memory pool is not full      --> padding 0\n",
    "        situation 2: cur_idx < frame_history_len and memory pool is full          --> no padding\n",
    "        situation 3: appear \"stop\" flag (check from end to start)                 --> padding 0\n",
    "        situation 4: other                                                        --> no padding\n",
    "\n",
    "        :return: idx_flag, missing_context, start_idx, end_idx\n",
    "        \"\"\"\n",
    "        end_idx = cur_idx + 1  # exclusive\n",
    "        start_idx = end_idx - self.frame_history_len  # inclusive\n",
    "        is_sit_3 = False\n",
    "\n",
    "        # situation 1 or 2 or 3\n",
    "        if start_idx < 0:\n",
    "            start_idx = 0\n",
    "            missing_context = self.frame_history_len - (end_idx - start_idx)\n",
    "\n",
    "            # situation 1\n",
    "            if self.number_of_memories != self.pool_size:\n",
    "                # not check end frame\n",
    "                for idx in range(start_idx, end_idx-1):\n",
    "                    # 0, 1|, 0, 0|, ...\n",
    "                    if self.memories[idx % self.pool_size]['done']:\n",
    "                        start_idx = idx + 1\n",
    "                        is_sit_3 = True\n",
    "\n",
    "                    if is_sit_3:\n",
    "                        missing_context = self.frame_history_len - (end_idx - start_idx)\n",
    "                        return 3, missing_context, start_idx, end_idx\n",
    "\n",
    "                return 1, missing_context, start_idx, end_idx\n",
    "\n",
    "            # situation 2\n",
    "            else:\n",
    "                for idx in range(start_idx, end_idx-1):\n",
    "                    if self.memories[idx % self.pool_size]['done']:\n",
    "                        start_idx = idx + 1\n",
    "                        is_sit_3 = True\n",
    "\n",
    "                    if is_sit_3:\n",
    "                        missing_context = self.frame_history_len - (end_idx - start_idx)\n",
    "                        return 3, missing_context, start_idx, end_idx\n",
    "\n",
    "                # not check end frame\n",
    "                for i in range(missing_context, 0, -1):\n",
    "                    idx = self.pool_size - i\n",
    "                    if self.memories[idx % self.pool_size]['done']:\n",
    "                        start_idx = (idx + 1) % self.pool_size\n",
    "                        is_sit_3 = True\n",
    "\n",
    "                    if is_sit_3:\n",
    "                        # ..., end_idx|, ..., |start_idx, ., end\n",
    "                        if start_idx > end_idx:\n",
    "                            missing_context = self.frame_history_len - (self.pool_size - start_idx + end_idx)\n",
    "                        else:\n",
    "                            missing_context = self.frame_history_len - (end_idx - start_idx)\n",
    "                        return 3, missing_context, start_idx, end_idx\n",
    "\n",
    "                start_idx = self.pool_size - missing_context\n",
    "                return 2, 0, start_idx, end_idx\n",
    "\n",
    "        # situation 3: appear \"stop\" flag\n",
    "        for idx in range(start_idx, end_idx-1):\n",
    "            if self.memories[idx % self.pool_size]['done']:\n",
    "                start_idx = idx + 1\n",
    "                is_sit_3 = True\n",
    "\n",
    "            if is_sit_3:\n",
    "                missing_context = self.frame_history_len - (end_idx - start_idx)\n",
    "                return 3, missing_context, start_idx, end_idx\n",
    "\n",
    "        return 4, 0, start_idx, end_idx\n",
    "\n",
    "    def _encoder_observation(self, cur_idx):\n",
    "        \"\"\"\n",
    "        concatenate recent \"frame_history_len\" frames\n",
    "        obs: (c, h, w) => (frame_history_len*c, h, w)\n",
    "        :param cur_idx: current frame's index\n",
    "        :return: tensor\n",
    "        \"\"\"\n",
    "\n",
    "        encoded_observation = []\n",
    "\n",
    "        idx_flag, missing_context, start_idx, end_idx = self._check_idx(cur_idx)\n",
    "\n",
    "        if missing_context > 0:\n",
    "            for i in range(missing_context):\n",
    "                encoded_observation.append(np.zeros_like(self.memories[0]['obs']))\n",
    "\n",
    "        # situation 3 in situation 2\n",
    "        if start_idx > end_idx:\n",
    "            for idx in range(start_idx, self.pool_size):\n",
    "                encoded_observation.append(self.memories[idx % self.pool_size]['obs'])\n",
    "            for idx in range(end_idx):\n",
    "                encoded_observation.append(self.memories[idx % self.pool_size]['obs'])\n",
    "        else:\n",
    "            for idx in range(start_idx, end_idx):\n",
    "                encoded_observation.append(self.memories[idx % self.pool_size]['obs'])\n",
    "\n",
    "        # encoded_observation: [k, c, h, w] => [k*c, h, w]\n",
    "        encoded_observation = np.concatenate(encoded_observation, 0)\n",
    "        return encoded_observation\n",
    "\n",
    "    def encoder_recent_observation(self):\n",
    "        \"\"\"\n",
    "        concatenate recent \"frame_history_len\" frames\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        assert self.number_of_memories > 0\n",
    "\n",
    "        current_idx = self.next_idx - 1\n",
    "        # when next_idx == 0\n",
    "        if current_idx < 0:\n",
    "            current_idx = self.pool_size - 1\n",
    "\n",
    "        return self._encoder_observation(current_idx)\n",
    "\n",
    "    def sample_memories(self, batch_size):\n",
    "        \"\"\"\n",
    "        choose randomly \"batch_size\" memories (batch_size, )\n",
    "        :param batch_size:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # ensure s_{i+1} is exist\n",
    "        sample_idxs = np.random.randint(0, self.number_of_memories-1, [batch_size])\n",
    "\n",
    "        # [batch_size, frame_history_len*c, h, w]\n",
    "        obs_batch = np.zeros(\n",
    "            [batch_size, self.obs_shape[0] * self.frame_history_len, self.obs_shape[1], self.obs_shape[2]])\n",
    "        next_obs_batch = np.copy(obs_batch)\n",
    "        action_batch = np.zeros([batch_size, 1])  # [batch_size, ]\n",
    "        reward_batch = np.zeros([batch_size, 1])  # [batch_size, ]\n",
    "        done_batch = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            obs_batch[i] = self._encoder_observation(sample_idxs[i])\n",
    "            next_obs_batch[i] = self._encoder_observation(sample_idxs[i] + 1)\n",
    "            action_batch[i] = self.memories[sample_idxs[i]]['action']\n",
    "            reward_batch[i] = self.memories[sample_idxs[i]]['reward']\n",
    "            done_batch.append(self.memories[sample_idxs[i]]['done'])\n",
    "\n",
    "        return obs_batch, next_obs_batch, action_batch, reward_batch, done_batch\n",
    "\n",
    "    def store_memory_obs(self, frame):\n",
    "        \"\"\"\n",
    "        store observation of memory\n",
    "        :param frame: numpy array\n",
    "                      Array of shape (img_h, img_w, img_c) and dtype np.uint8\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # obs is a image (h, w, c)\n",
    "        frame = frame.transpose(2, 0, 1)  # c, w, h\n",
    "\n",
    "        if self.obs_shape is None:\n",
    "            self.obs_shape = frame.shape\n",
    "\n",
    "        if self.memories is None:\n",
    "            self.memories = [dict() for i in range(self.pool_size)]\n",
    "\n",
    "        self.memories[self.next_idx]['obs'] = frame\n",
    "        index = self.next_idx\n",
    "\n",
    "        self.next_idx = (self.next_idx + 1) % self.pool_size\n",
    "        self.number_of_memories = min([self.number_of_memories + 1, self.pool_size])\n",
    "\n",
    "        return index\n",
    "\n",
    "    def store_memory_effect(self, index, action, reward, done):\n",
    "        \"\"\"\n",
    "        store other information of memory\n",
    "        :param action: scalar\n",
    "        :param done: bool\n",
    "        :param reward: scalar\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.memories[index]['action'] = action\n",
    "        self.memories[index]['reward'] = reward\n",
    "        self.memories[index]['done'] = done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "42f35815-9ae6-45ad-b124-093fdb932a5e"
   },
   "source": [
    "## 神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "uuid": "03cb6414-ffe7-4899-b5bb-b0132976000a"
   },
   "outputs": [],
   "source": [
    "class DQNet(nn.Module):\n",
    "    def __init__(self, input_channels, out_channels):\n",
    "        super(DQNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_channels, out_channels=32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=7*7*64, out_features=512)\n",
    "        self.fc2 = nn.Linear(in_features=512, out_features=out_channels)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "uuid": "ff8e727b-cccb-43eb-9da4-b9f5657040a2"
   },
   "outputs": [],
   "source": [
    "def change_to_tensor(data_np, dtype=torch.float32):\n",
    "    \"\"\"\n",
    "    change numpy array to torch.tensor\n",
    "    :param dtype:\n",
    "    :param data_np:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    data_tensor = torch.from_numpy(data_np).type(dtype)\n",
    "    if torch.cuda.is_available():\n",
    "        data_tensor = data_tensor.cuda()\n",
    "    return data_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "aa5e4444-e3b2-4bf9-961c-7a04d5cc53b6"
   },
   "source": [
    "## Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "uuid": "b4336c8f-1370-4004-a61b-470609856638"
   },
   "outputs": [],
   "source": [
    "class PreprocessFrame(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    preprocess the observation of env\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super(PreprocessFrame, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1))\n",
    "\n",
    "    def observation(self, observation):\n",
    "        \"\"\"\n",
    "        raw data: [210, 160, 3]\n",
    "        processed data: [84, 84, 1]\n",
    "        :param observation:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        img = np.reshape(observation, [210, 160, 3]).astype(np.float32)\n",
    "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
    "        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_LINEAR)\n",
    "        x_t = resized_screen[18:102, :]\n",
    "        x_t = np.reshape(x_t, [84, 84, 1])\n",
    "        return x_t.astype(np.uint8)\n",
    "\n",
    "\n",
    "# one life one episode\n",
    "class EpisodicLifeEnv(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n",
    "        Done by DeepMind for the DQN and co. since it helps value estimation.\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.lives = 0\n",
    "        self.was_real_done = True\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        self.was_real_done = done\n",
    "        # check current lives, make loss of life terminal,\n",
    "        # then update lives to handle bonus lives\n",
    "        lives = self.env.unwrapped.ale.lives()\n",
    "        if self.lives > lives > 0:\n",
    "            # for Qbert sometimes we stay in lives == 0 condition for a few frames\n",
    "            # so it's important to keep lives > 0, so that we only reset once\n",
    "            # the environment advertises done.\n",
    "            done = True\n",
    "        self.lives = lives\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"Reset only when lives are exhausted.\n",
    "        This way all states are still reachable even though lives are episodic,\n",
    "        and the learner need not know about any of this behind-the-scenes.\n",
    "        \"\"\"\n",
    "        if self.was_real_done:\n",
    "            obs = self.env.reset(**kwargs)\n",
    "        else:\n",
    "            # no-op step to advance from terminal/lost life state\n",
    "            obs, _, _, _ = self.env.step(0)\n",
    "        self.lives = self.env.unwrapped.ale.lives()\n",
    "        return obs\n",
    "\n",
    "\n",
    "class NoopResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env, noop_max=30):\n",
    "        \"\"\"Sample initial states by taking random number of no-ops on reset.\n",
    "        No-op is assumed to be action 0.\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.noop_max = noop_max\n",
    "        self.override_num_noops = None\n",
    "        self.noop_action = 0\n",
    "        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
    "        self.env.reset(**kwargs)\n",
    "        if self.override_num_noops is not None:\n",
    "            noops = self.override_num_noops\n",
    "        else:\n",
    "            noops = self.unwrapped.np_random.randint(1, self.noop_max + 1)\n",
    "        assert noops > 0\n",
    "        obs = None\n",
    "        for _ in range(noops):\n",
    "            obs, _, done, _ = self.env.step(self.noop_action)\n",
    "            if done:\n",
    "                obs = self.env.reset(**kwargs)\n",
    "        return obs\n",
    "\n",
    "    def step(self, ac):\n",
    "        return self.env.step(ac)\n",
    "\n",
    "\n",
    "class FireResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"Take action on reset for environments that are fixed until firing.\"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
    "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        self.env.reset(**kwargs)\n",
    "        obs, _, done, _ = self.env.step(1)\n",
    "        if done:\n",
    "            self.env.reset(**kwargs)\n",
    "        obs, _, done, _ = self.env.step(2)\n",
    "        if done:\n",
    "            self.env.reset(**kwargs)\n",
    "        return obs\n",
    "\n",
    "    def step(self, ac):\n",
    "        return self.env.step(ac)\n",
    "\n",
    "\n",
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env, skip=4):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        # most recent raw observations (for max pooling across time steps)\n",
    "        self._obs_buffer = np.zeros((2,) + env.observation_space.shape, dtype=np.uint8)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            if i == self._skip - 2:\n",
    "                self._obs_buffer[0] = obs\n",
    "            if i == self._skip - 1:\n",
    "                self._obs_buffer[1] = obs\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        # Note that the observation on the done=True frame\n",
    "        # doesn't matter\n",
    "        max_frame = np.max(self._obs_buffer, axis=0)\n",
    "        # max_frame = self._obs_buffer.max(axis=0)\n",
    "\n",
    "        return max_frame, total_reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "\n",
    "class ClipRewardEnv(gym.RewardWrapper):\n",
    "    def __init__(self, env):\n",
    "        gym.RewardWrapper.__init__(self, env)\n",
    "\n",
    "    def reward(self, reward):\n",
    "        \"\"\"Bin reward to {+1, 0, -1} by its sign.\"\"\"\n",
    "        return np.sign(reward)\n",
    "\n",
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=env.observation_space.shape, dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        # careful! This undoes the memory optimization, use\n",
    "        # with smaller replay buffers only.\n",
    "        return np.array(observation).astype(np.float32) / 255.0\n",
    "\n",
    "\n",
    "def wrap_deepmind(env, skip=4, no_op_max=30, episode_life=True, clip_rewards=True, scale=True):\n",
    "    \"\"\"Configure environment for DeepMind-style Atari.\n",
    "    \"\"\"\n",
    "\n",
    "    # one life one episode to speed train\n",
    "    if episode_life:\n",
    "        env = EpisodicLifeEnv(env)\n",
    "\n",
    "    # after reset taking random number of no-ops:\n",
    "    env = NoopResetEnv(env, no_op_max)\n",
    "\n",
    "    # use same action in k frames, compute all rewards, maximum last two frames\n",
    "    env = MaxAndSkipEnv(env, skip)\n",
    "\n",
    "    # return an env which would not over after using \"fire\"\n",
    "    if 'FIRE' in env.unwrapped.get_action_meanings():\n",
    "        env = FireResetEnv(env)\n",
    "\n",
    "    env = PreprocessFrame(env)\n",
    "\n",
    "    if scale:\n",
    "        env = ScaledFloatFrame(env)\n",
    "\n",
    "    if clip_rewards:\n",
    "        env = ClipRewardEnv(env)\n",
    "\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "caa59845-838f-4886-924f-8a3368507886"
   },
   "source": [
    "## trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "uuid": "50cf6367-3ded-4ed8-a901-60267cb49d2c"
   },
   "outputs": [],
   "source": [
    "class DQN_trainer:\n",
    "    def __init__(self, config):\n",
    "        # env info\n",
    "        self.env = wrap_deepmind(gym.make('Breakout-v0'), skip=config.action_repeat, no_op_max=config.no_op_max)\n",
    "        if config.is_monitor:\n",
    "            self.env = gym.wrappers.Monitor(self.env, 'recording')\n",
    "        self.action_num = self.env.action_space.n\n",
    "        self.obs_shape = self.env.observation_space.shape  # [h, w, c]\n",
    "        self.last_obs = self.env.reset()\n",
    "\n",
    "        # reply buffer\n",
    "        self.reply_buffer = ReplayBuffer(config.replay_memory_size, config.agent_history_length)\n",
    "\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # initial model [batch_size, h, w, m*c]\n",
    "        self.eval_model = DQNet(self.obs_shape[2] * config.agent_history_length, self.action_num).to(self.device)\n",
    "        self.target_model = DQNet(self.obs_shape[2] * config.agent_history_length, self.action_num).to(self.device)\n",
    "        \n",
    "        if True:\n",
    "            self.eval_model.load_state_dict(torch.load('./checkpoints/state_dict_step900000_ave_reward_2.5945.pth'))\n",
    "            self.target_model.load_state_dict(torch.load('./checkpoints/state_dict_step900000_ave_reward_2.5945.pth'))\n",
    "            \n",
    "        # train param\n",
    "        self.exploration = np.linspace(config.initial_exploration, config.final_exploration,\n",
    "                                       config.final_exploration_frame)\n",
    "        self.final_exploration_frame = config.final_exploration_frame\n",
    "        self.discount_factor = config.discount_factor\n",
    "        self.max_epoch = config.max_epoch\n",
    "        self.learning_starts = config.learning_starts\n",
    "        self.update_freq = config.update_freq\n",
    "        self.target_update_freq = config.target_update_freq\n",
    "        self.batch_size = config.batch_size\n",
    "\n",
    "        self.model_path = config.model_path\n",
    "        self.load_model_freq = config.load_model_freq\n",
    "\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.eval_model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "        # self.viz = visdom.Visdom(env=\"DQN_train\", log_to_filename=\"./logs/viz_dqn_train.log\")\n",
    "        self.log_freq = config.log_freq\n",
    "\n",
    "    def collect_memories(self):\n",
    "        \"\"\"\n",
    "        before DQN begins to learn, collect adequate memories.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        print(\"-------------------collect memories------------------------\")\n",
    "        for step in tqdm(range(self.learning_starts)):\n",
    "            # store observation\n",
    "            cur_index = self.reply_buffer.store_memory_obs(self.last_obs)\n",
    "            # choose action randomly\n",
    "            action = self.env.action_space.sample()\n",
    "            # interact with env\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            # clip reward\n",
    "            reward = np.clip(reward, -1.0, 1.0)\n",
    "            # store other info\n",
    "            self.reply_buffer.store_memory_effect(cur_index, action, reward, done)\n",
    "\n",
    "            if done:\n",
    "                obs = self.env.reset()\n",
    "\n",
    "            self.last_obs = obs\n",
    "        print(\"---------------------------end-----------------------------\")\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        train DQN agent\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        total_reward = 0\n",
    "        total_step = 0\n",
    "        total_average_reward = 0\n",
    "        total_average_step = 0\n",
    "        episode = 0\n",
    "        episode_1000 = 0\n",
    "        logout = False\n",
    "\n",
    "        train_ave_loss = 0\n",
    "        log_step = 0\n",
    "\n",
    "        self.last_obs = self.env.reset()\n",
    "        print(\"-------------------train DQN agent------------------------\")\n",
    "        for step in tqdm(range(1, self.max_epoch)):\n",
    "            cur_index = self.reply_buffer.store_memory_obs(self.last_obs)\n",
    "            encoded_obs = self.reply_buffer.encoder_recent_observation()  # numpy: [m*c, h, w]\n",
    "\n",
    "#             # visualize last k frames\n",
    "#             image_num = int(encoded_obs.shape[0] / self.obs_shape[2])\n",
    "#             images_numpy = np.array([[encoded_obs[i]] for i in range(image_num)])\n",
    "#             self.viz.images(torch.from_numpy(images_numpy), win=\"observations\")\n",
    "\n",
    "            sample = np.random.random()\n",
    "            # change from 1.0 to 0.1 linearly\n",
    "#             epsilon = self.exploration[min([step, self.final_exploration_frame])]\n",
    "            if sample > 0.1:\n",
    "                # numpy: [m*c, h, w] => tensor: [1, m*c, h, w]\n",
    "                encoded_obs = change_to_tensor(encoded_obs).unsqueeze(0)\n",
    "                pred_action_values = self.eval_model(encoded_obs)  # [1, 4]\n",
    "                _, action = pred_action_values.max(dim=1)\n",
    "                action = action.item()\n",
    "            else:\n",
    "                action = self.env.action_space.sample()\n",
    "\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "\n",
    "            total_reward += reward\n",
    "            total_step += 1\n",
    "\n",
    "            # reward = np.clip(reward, -1.0, 1.0)\n",
    "\n",
    "            self.reply_buffer.store_memory_effect(cur_index, action, reward, done)\n",
    "\n",
    "            if done:\n",
    "                obs = self.env.reset()\n",
    "                episode += 1\n",
    "                if (episode % 1000) == 0:\n",
    "                    logout = True\n",
    "                total_average_reward += total_reward\n",
    "                total_average_step += total_step\n",
    "                total_reward = 0\n",
    "                total_step = 0\n",
    "\n",
    "            self.last_obs = obs\n",
    "\n",
    "            # train the model\n",
    "            if step % self.update_freq == 0 and step > self.learning_starts:\n",
    "                obs_batch, next_obs_batch, action_batch, reward_batch, done_batch = self.reply_buffer.sample_memories(\n",
    "                    self.batch_size)\n",
    "                # numpy to tensor\n",
    "                obs_batch, next_obs_batch = change_to_tensor(obs_batch), change_to_tensor(next_obs_batch)\n",
    "                action_batch, reward_batch = change_to_tensor(action_batch, torch.int64), change_to_tensor(reward_batch)\n",
    "\n",
    "                # estimate Q values\n",
    "                q_values = self.eval_model(obs_batch)  # [b, action_num]\n",
    "                q_pred = q_values.gather(dim=1, index=action_batch)  # [b, 1]\n",
    "\n",
    "                # target Q values\n",
    "                q_next = self.target_model(next_obs_batch).detach()\n",
    "                # Bellman equation\n",
    "                q_target = reward_batch + self.discount_factor * q_next.max(dim=1)[0].view(self.batch_size, -1)\n",
    "\n",
    "                loss = self.criterion(q_pred, q_target)\n",
    "                train_ave_loss += loss.item()\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            # update target net\n",
    "            if step % self.target_update_freq == 0 and step > self.learning_starts:\n",
    "                self.target_model.load_state_dict(OrderedDict(self.eval_model.state_dict()))\n",
    "\n",
    "            # save model\n",
    "            if (step / self.update_freq) % self.load_model_freq == 0 and step > self.learning_starts:\n",
    "                torch.save(self.eval_model.state_dict(), self.model_path + '/state_dict_step%d-trainLoss%.4f.pth' % (step, loss.item()*1000))\n",
    "\n",
    "#             # visualize data\n",
    "#             if (step / self.update_freq) % self.log_freq == 0:\n",
    "#                 log_step += 1\n",
    "#                 train_ave_loss = train_ave_loss / self.log_freq\n",
    "#                 self.viz.line([train_ave_loss], [log_step], win='train_average_loss', update='append', opts=dict(\n",
    "#                                 title=\"train_average_loss\",\n",
    "#                                 xlabel=\"log_step\",\n",
    "#                                 ylabel=\"average_loss\"\n",
    "#                             ))\n",
    "#                 train_ave_loss = 0\n",
    "\n",
    "            if episode % 1000 == 0 and logout:\n",
    "                logout = False\n",
    "                episode_1000 += 1\n",
    "                total_average_reward = total_average_reward / 1000\n",
    "                total_average_step = total_average_step / 1000\n",
    "                \n",
    "                print(\"---------------------------end-----------------------------\")\n",
    "                print(\"average reward in recent 1000 times: {}\".format(total_average_reward))\n",
    "                print(\"average step in recent 1000 times: {}\".format(total_average_step))\n",
    "                print(\"---------------------------end-----------------------------\")\n",
    "#                 self.viz.line([total_ave100_reward], [episode_100], win='average100_reward', update='append', opts=dict(\n",
    "#                     title=\"average100_reward\",\n",
    "#                     xlabel=\"episode_100\",\n",
    "#                     ylabel=\"average_reward\"\n",
    "#                 ))\n",
    "#                 self.viz.line([total_ave100_step], [episode_100], win='average100_step', update='append', opts=dict(\n",
    "#                     title=\"average100_step\",\n",
    "#                     xlabel=\"episode_100\",\n",
    "#                     ylabel=\"average_step\"\n",
    "#                 ))\n",
    "\n",
    "        print(\"---------------------------end-----------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "24ca3909-2c64-4bda-bd16-8565de7bef3d"
   },
   "source": [
    "## load argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "uuid": "c5a8d335-3708-482f-b55a-c32fccac37a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(action_repeat=4, agent_history_length=4, batch_size=32, discount_factor=0.99, final_exploration=0.1, final_exploration_frame=1000000, gradient_momentum=0.95, initial_exploration=1.0, is_monitor=False, learning_rate=0.00025, learning_starts=5000, load_model_freq=12500, log_freq=1000, max_epoch=8000000, min_squared_gradient=0.01, model_path='./checkpoints/state_dict_step900000_ave_reward_2.5945.pth', no_op_max=30, replay_memory_size=1000000, squared_gradient_momentum=0.95, target_update_freq=100000, update_freq=4)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# paper \"Human-level control through deep reinforcement learning\" argument\n",
    "parser.add_argument('--batch_size', type=int, default=32)\n",
    "parser.add_argument('--replay_memory_size', type=int, default=1000000)\n",
    "parser.add_argument('--agent_history_length', type=int, default=4)\n",
    "parser.add_argument('--target_update_freq', type=int, default=100000, help='target net update its param')\n",
    "parser.add_argument('--discount_factor', type=float, default=0.99, help='discount factor')\n",
    "parser.add_argument('--action_repeat', type=int, default=4, help='repeat same action in k frames')\n",
    "parser.add_argument('--update_freq', type=int, default=4, help='DQN learn once per learning freq')\n",
    "# RMSProp\n",
    "parser.add_argument('--learning_rate', type=float, default=0.00025)\n",
    "parser.add_argument('--gradient_momentum', type=float, default=0.95)\n",
    "parser.add_argument('--squared_gradient_momentum', type=float, default=0.95)\n",
    "parser.add_argument('--min_squared_gradient', type=float, default=0.01)\n",
    "# epsilon\n",
    "parser.add_argument('--initial_exploration', type=float, default=1.0)\n",
    "parser.add_argument('--final_exploration', type=float, default=0.1)\n",
    "parser.add_argument('--final_exploration_frame', type=int, default=1000000)\n",
    "\n",
    "parser.add_argument('--learning_starts', type=int, default=5000, help='after learning starts DQN begin to learn')\n",
    "parser.add_argument('--no_op_max', type=int, default=30, help='after reset taking random number of no-ops')\n",
    "\n",
    "parser.add_argument('--load_model_freq', type=int, default=12500)\n",
    "parser.add_argument('--model_path', type=str, default='./checkpoints', help='path for saving trained models')\n",
    "\n",
    "# other argument\n",
    "parser.add_argument('--max_epoch', type=int, default=8000000)\n",
    "parser.add_argument('--is_monitor', type=bool, default=False, help='use monitor log the performance of the agent')\n",
    "parser.add_argument('--log_freq', type=int, default=1000, help='step size for updating visdom')\n",
    "\n",
    "args = parser.parse_known_args()[0]\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "uuid": "d0bb97bc-7b3c-4ffb-b4df-4a1936ffedf1"
   },
   "outputs": [],
   "source": [
    "dqn_agent_trainer = DQN_trainer(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "uuid": "1d7dd5a4-64e4-4da9-980e-e637ebcd9720"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.DQN_trainer object at 0x0000028401A748C8>\n"
     ]
    }
   ],
   "source": [
    "print(dqn_agent_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "uuid": "7f3b2e67-8028-4f42-87c7-24fc22aff4d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------collect memories------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:52<00:00, 95.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------end-----------------------------\n",
      "-------------------train DQN agent------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                        | 6478/7999999 [01:09<32:03:29, 69.26it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-b0e967cc933c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdqn_agent_trainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect_memories\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdqn_agent_trainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-6f0a196dd144>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    104\u001b[0m                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m             \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[0mtotal_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Software\\Anaconda\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 280\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    281\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Software\\Anaconda\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Software\\Anaconda\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-c52b264e2c2c>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, ac)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mac\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mac\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-c52b264e2c2c>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_skip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m             \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_skip\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_obs_buffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-c52b264e2c2c>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, ac)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mac\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mac\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-c52b264e2c2c>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwas_real_done\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;31m# check current lives, make loss of life terminal,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Software\\Anaconda\\lib\\site-packages\\gym\\wrappers\\time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Software\\Anaconda\\lib\\site-packages\\gym\\envs\\atari\\atari_env.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, a)\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[0mnum_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m             \u001b[0mreward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0male\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m         \u001b[0mob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Software\\Anaconda\\lib\\site-packages\\atari_py\\ale_python_interface.py\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0male_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgame_over\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dqn_agent_trainer.collect_memories()\n",
    "dqn_agent_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "uuid": "d6aba987-3d06-4dea-8d77-c7a291c24c08"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
